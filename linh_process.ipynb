{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation=nn.GELU):\n",
    "        super(Stem, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, output_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(output_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(output_dim // 2, output_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "            nn.GELU()   \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.stem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileViG(torch.nn.Module):\n",
    "    def __init__(self, local_channels):\n",
    "        super(MobileViG, self).__init__()\n",
    "        self.stem = Stem(input_dim=3, output_dim=local_channels[0])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.stem(inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img: np.ndarray, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) -> torch.Tensor:\n",
    "    preprocessing = Compose([ToTensor(), Normalize(mean=mean, std=std)])\n",
    "    return preprocessing(img.copy()).unsqueeze(0)\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "    img = img * 0.1 + 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return np.uint8(img * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map 0 is saved\n",
      "feature map 1 is saved\n",
      "feature map 2 is saved\n",
      "feature map 3 is saved\n",
      "feature map 4 is saved\n",
      "feature map 5 is saved\n",
      "feature map 6 is saved\n",
      "feature map 7 is saved\n",
      "feature map 8 is saved\n",
      "feature map 9 is saved\n",
      "feature map 10 is saved\n",
      "feature map 11 is saved\n",
      "feature map 12 is saved\n",
      "feature map 13 is saved\n",
      "feature map 14 is saved\n",
      "feature map 15 is saved\n",
      "feature map 16 is saved\n",
      "feature map 17 is saved\n",
      "feature map 18 is saved\n",
      "feature map 19 is saved\n",
      "feature map 20 is saved\n",
      "feature map 21 is saved\n",
      "feature map 22 is saved\n",
      "feature map 23 is saved\n",
      "feature map 24 is saved\n",
      "feature map 25 is saved\n",
      "feature map 26 is saved\n",
      "feature map 27 is saved\n",
      "feature map 28 is saved\n",
      "feature map 29 is saved\n",
      "feature map 30 is saved\n",
      "feature map 31 is saved\n",
      "feature map 32 is saved\n",
      "feature map 33 is saved\n",
      "feature map 34 is saved\n",
      "feature map 35 is saved\n",
      "feature map 36 is saved\n",
      "feature map 37 is saved\n",
      "feature map 38 is saved\n",
      "feature map 39 is saved\n",
      "feature map 40 is saved\n",
      "feature map 41 is saved\n"
     ]
    }
   ],
   "source": [
    " import torchsummary\n",
    "# test model with (1, 3, 224, 224) input \n",
    "checkpoint_path = 'MobileViG_B_82_6.pth.tar'\n",
    "checkpoint = torch.load(f'./{checkpoint_path}', map_location='cpu')\n",
    "\n",
    "img = cv2.imread('./image_test/demo.jpg')\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = np.float32(img) / 255\n",
    "\n",
    "img = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "local_channels=[42, 84, 240]\n",
    "model = MobileViG(local_channels)\n",
    "state_dict = {k:v for k,v in checkpoint['state_dict'].items() if 'stem' in k}\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "out = model(img)\n",
    "out = out.detach().numpy()\n",
    "out = out[0]\n",
    "\n",
    "# visualize feature map\n",
    "\n",
    "for i in range(len(out)):\n",
    "    img_result = deprocess_image(out[i])\n",
    "    color_img = cv2.cvtColor(img_result, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.imwrite(f'./test/feature_map_{i}.jpg', color_img)\n",
    "    print(f'feature map {i} is saved')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
